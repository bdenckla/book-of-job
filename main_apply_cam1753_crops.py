# Initially generated by GitHub Copilot.
"""
Apply cam1753 word crops from exported editor JSON.

Reads the bbox JSON exported by the cam1753 crop editor (from clipboard
or .novc/cam1753_crops_export.json), crops each word from the full-resolution
page image, and saves to docs/jobn/img/cam1753/cam1753-{sid}.png.

Each output PNG includes tEXt metadata:
  - ``cam1753-source``: JSON from the source JPEG EXIF (spread info)
  - ``cam1753-crop``: JSON with the crop editor export fields

Each batch is also appended to ``docs/jobn/img/cam1753/cam1753-crops.json``,
a persistent record of all crop coordinates keyed by SID. This file stores
enough data to reproduce crops programmatically at any image resolution.

Usage:
    .venv\\Scripts\\python.exe main_apply_cam1753_crops.py .novc/cam1753_crops_export.json
"""

import json
import sys
from pathlib import Path

from PIL import Image
from PIL.PngImagePlugin import PngInfo

ROOT = Path(__file__).resolve().parent

# Add sibling repo to sys.path
CAM1753_REPO = ROOT.parent / "codex-index-cam1753"
sys.path.insert(0, str(CAM1753_REPO))

from py_cam1753_word_image.page import load_page_image

OUT_DIR = ROOT / "docs" / "jobn" / "img" / "cam1753"
CROPS_JSON = ROOT / "out" / "cam1753-crops.json"


def _get_source_metadata(img):
    """Extract the JSON string from EXIF tag 270 (ImageDescription)."""
    exif_bytes = img.info.get("exif")
    if not exif_bytes:
        return None
    try:
        from PIL.ExifTags import Base as ExifBase

        exif_data = img.getexif()
        return exif_data.get(ExifBase.ImageDescription)
    except Exception:
        return None


def _load_crops_json():
    """Load the persistent crops JSON (keyed by SID), or empty dict."""
    if CROPS_JSON.exists():
        return json.loads(CROPS_JSON.read_text("utf-8"))
    return {}


def _save_crops_json(data):
    """Write the persistent crops JSON, sorted by SID."""
    sorted_data = dict(sorted(data.items()))
    CROPS_JSON.write_text(
        json.dumps(sorted_data, indent=2, ensure_ascii=False) + "\n",
        encoding="utf-8",
    )


def _ensure_page(page_id, page_cache, source_meta_cache):
    """Load and cache a page image if not already loaded."""
    if page_id not in page_cache:
        print(f"  Loading page {page_id}...")
        page_cache[page_id] = load_page_image(page_id)
        source_meta_cache[page_id] = _get_source_metadata(page_cache[page_id])


def _apply_normal_crop(crop, page_cache, source_meta_cache, persistent):
    """Crop a single (non-split) word and save the PNG."""
    sid = crop["sid"]
    page_id = crop["page"]
    bb = crop["bbox_abs"]
    x, y, w, h = bb["x"], bb["y"], bb["w"], bb["h"]

    _ensure_page(page_id, page_cache, source_meta_cache)
    img = page_cache[page_id]
    page_w, page_h = img.size
    cropped = img.crop((x, y, x + w, y + h))

    # Build PNG metadata
    png_info = PngInfo()
    source_meta_str = source_meta_cache[page_id]
    if source_meta_str:
        png_info.add_text("cam1753-source", source_meta_str)
    crop_meta = {k: v for k, v in crop.items() if k != "sid"}
    png_info.add_text("cam1753-crop", json.dumps(crop_meta))

    out_path = OUT_DIR / f"cam1753-{sid}.png"
    cropped.save(out_path, pnginfo=png_info)
    print(f"  {out_path.name}: {cropped.size[0]}\u00d7{cropped.size[1]}")

    # Persistent record
    source_meta = json.loads(source_meta_str) if source_meta_str else None
    persistent[sid] = {
        "cv": crop["cv"],
        "page": page_id,
        "col": crop["col"],
        "line_num": crop["line_num"],
        "word_idx": crop.get("word_idx"),
        "page_size": [page_w, page_h],
        "bbox_abs": crop["bbox_abs"],
        "bbox_rel": crop["bbox_rel"],
    }
    if source_meta:
        persistent[sid]["source"] = source_meta
    return 1


def _apply_split_crop(crop, page_cache, source_meta_cache, persistent):
    """Crop a cross-line maqaf split word (two parts) and combine into one PNG.

    Hebrew is RTL, so the combined image places part A (the leading maqaf
    fragment from the end of the previous line) on the *right*, and part B
    (the trailing fragment from the start of the next line) on the *left*.
    """
    sid = crop["sid"]
    page_id = crop["page"]
    parts = crop["parts"]

    _ensure_page(page_id, page_cache, source_meta_cache)
    img = page_cache[page_id]
    page_w, page_h = img.size

    # Crop each part
    part_images = []
    for p in parts:
        bb = p["bbox_abs"]
        x, y, w, h = bb["x"], bb["y"], bb["w"], bb["h"]
        part_img = img.crop((x, y, x + w, y + h))
        part_images.append(part_img)

    # Combine: part A (idx 0, maqaf-ending) on right, part B (idx 1) on left
    img_a, img_b = part_images[0], part_images[1]
    gap = 4  # small gap between parts
    combined_w = img_a.width + gap + img_b.width
    combined_h = max(img_a.height, img_b.height)
    combined = Image.new("RGB", (combined_w, combined_h), (255, 255, 255))
    # Part B on the left
    y_off_b = (combined_h - img_b.height) // 2
    combined.paste(img_b, (0, y_off_b))
    # Part A on the right
    y_off_a = (combined_h - img_a.height) // 2
    combined.paste(img_a, (img_b.width + gap, y_off_a))

    # Build PNG metadata
    png_info = PngInfo()
    source_meta_str = source_meta_cache[page_id]
    if source_meta_str:
        png_info.add_text("cam1753-source", source_meta_str)
    crop_meta = {k: v for k, v in crop.items() if k != "sid"}
    png_info.add_text("cam1753-crop", json.dumps(crop_meta))

    out_path = OUT_DIR / f"cam1753-{sid}.png"
    combined.save(out_path, pnginfo=png_info)
    print(
        f"  {out_path.name}: {combined.size[0]}\u00d7{combined.size[1]} "
        f"(split: {img_a.size[0]}\u00d7{img_a.size[1]} + "
        f"{img_b.size[0]}\u00d7{img_b.size[1]})"
    )

    # Persistent record
    source_meta = json.loads(source_meta_str) if source_meta_str else None
    persistent[sid] = {
        "cv": crop["cv"],
        "page": page_id,
        "col": crop["col"],
        "split": True,
        "parts": [
            {
                "line_num": p["line_num"],
                "word_idx": p.get("word_idx"),
                "label": p["label"],
                "bbox_abs": p["bbox_abs"],
                "bbox_rel": p["bbox_rel"],
            }
            for p in parts
        ],
        "page_size": [page_w, page_h],
    }
    if source_meta:
        persistent[sid]["source"] = source_meta
    return 1


def main():
    if len(sys.argv) < 2:
        print("Usage: main_apply_cam1753_crops.py <json_file>")
        print("  The JSON file should contain the array exported by the crop editor.")
        sys.exit(1)

    json_path = Path(sys.argv[1])
    if not json_path.exists():
        print(f"File not found: {json_path}")
        sys.exit(1)

    crops = json.loads(json_path.read_text("utf-8"))
    print(f"Applying {len(crops)} crops...")

    persistent = _load_crops_json()
    page_cache = {}
    source_meta_cache = {}
    saved = 0

    for crop in crops:
        if crop.get("split"):
            saved += _apply_split_crop(crop, page_cache, source_meta_cache, persistent)
        else:
            saved += _apply_normal_crop(crop, page_cache, source_meta_cache, persistent)

    _save_crops_json(persistent)
    print(f"Done. {saved} images saved to {OUT_DIR}")
    print(f"Persistent crop data: {CROPS_JSON} ({len(persistent)} entries)")


if __name__ == "__main__":
    main()
